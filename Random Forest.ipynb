{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sort of an extension of Decision \n",
    "\n",
    "Major Drawback of Decision Trees is overfitting, so in random forest we have a forest, that is a lot of decision trees together and we will take the majority in those.\n",
    "\n",
    "We will add some randomeness, that is some outliers,etc in the data to get multiple decision trees and to check for overfitting, hence the name Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging - Bootstrap Aggregation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need different decision trees from the same data points and features.\n",
    "\n",
    "Bagging helps us select decision trees and data points with the help of replacement. So basically if I have 50 data points, each with 100 features, bagging will help me select 50 data points but with replacement. That means I will have to select 50 Data points but with repetition. I can have 4 six times, 17 three times, etc. This will help us get multiple sets of data points from the original 50 data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Step 2, in the above we selected data points, but this helps in selection of features. That is, we dont select all features but only 'k' features out of 'n' features. Note : No repetition is allowed here like Bagging.\n",
    "\n",
    "The standard value for k is SQRT(N)\n",
    "\n",
    "We do this because there might be some features which cause overfitting, and by having multiple trees we can find out which trees are pure and not overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after doing all this if we feel that random forests are giving us overfitted data, we can add extra trees.\n",
    "\n",
    "We decided i/p points and features. This helps us decide which feature to split on or for a continous value which central value to choose to split on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Attached as RF.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
